{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, jit, grad, jacrev, value_and_grad\n",
    "import jax.numpy as jnp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from jax.scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return jnp.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers=[784, 300, 100, 10]):\n",
    "        \n",
    "        self.params = []\n",
    "        key = random.PRNGKey(0)\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "            # FIXME: Move me outside, random initialization\n",
    "            key_w, key_b = random.split(key, 2)\n",
    "            w = 1e-2 * random.normal(key_w, (layers[i+1], layers[i]))\n",
    "            b = 1e-2 * random.normal(key_b, (1, layers[i+1]))\n",
    "            \n",
    "            self.params.append([w, b])\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for w,b in self.params[:-1]:\n",
    "            x = jnp.dot(x, w.T) + b\n",
    "            x = relu(x)\n",
    "        \n",
    "        w,b = self.params[-1]\n",
    "        x = jnp.dot(x, w.T) + b\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(params, x, y):\n",
    "        \n",
    "        for w,b in params[:-1]:\n",
    "            x = jnp.dot(x, w.T) + b\n",
    "            x = relu(x)\n",
    "        \n",
    "        w,b = params[-1]\n",
    "        y_pred = jnp.dot(x, w.T) + b\n",
    "        \n",
    "        \n",
    "        # Apply log-softmax\n",
    "        sum_y_pred = logsumexp(y_pred, 1, keepdims=True)\n",
    "        y_pred = y_pred - sum_y_pred\n",
    "        \n",
    "        # Calculate loss\n",
    "        l = -jnp.sum(y*y_pred, 1)\n",
    "        \n",
    "        return jnp.mean(l)\n",
    "    \n",
    "\n",
    "    def update(self, grads, lr = 0.01):\n",
    "        for g,p in zip(grad, self.params):\n",
    "            p[0] = p[0] - lr * g[0]\n",
    "            p[1] = p[1] - lr * g[1]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
    "X, Y = mnist_train\n",
    "X_t, Y_t = mnist_test\n",
    "\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "X_t = X_t.reshape(X_t.shape[0], -1)\n",
    "\n",
    "X = X / 255\n",
    "X_t = X_t / 255\n",
    "\n",
    "Y = tf.keras.utils.to_categorical(Y)\n",
    "Y_t = tf.keras.utils.to_categorical(Y_t)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "print(X_t.shape)\n",
    "print(Y_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:127: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 2.301670789718628\n",
      "Epoch 1 : 2.299506902694702\n",
      "Epoch 2 : 2.295412063598633\n",
      "Epoch 3 : 2.2795825004577637\n",
      "Epoch 4 : 2.1601624488830566\n",
      "Epoch 5 : 1.5485061407089233\n",
      "Epoch 6 : 0.9187452793121338\n",
      "Epoch 7 : 0.7305006384849548\n",
      "Epoch 8 : 0.6579647064208984\n",
      "Epoch 9 : 0.609839677810669\n",
      "Epoch 10 : 0.5654870867729187\n",
      "Epoch 11 : 0.5185030698776245\n",
      "Epoch 12 : 0.4745645821094513\n",
      "Epoch 13 : 0.44207873940467834\n",
      "Epoch 14 : 0.41690927743911743\n",
      "Epoch 15 : 0.39584100246429443\n",
      "Epoch 16 : 0.3778747022151947\n",
      "Epoch 17 : 0.3621681034564972\n",
      "Epoch 18 : 0.3482908606529236\n",
      "Epoch 19 : 0.33564209938049316\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch = 128\n",
    "lr = 0.01\n",
    "\n",
    "model = Network([784, 300, 100, 10])\n",
    "params = model.get_params()\n",
    "\n",
    "for i in range(epochs):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(X.shape[0])\n",
    "    train_iter = iter(train_dataset.batch(batch))\n",
    "    \n",
    "    running_loss = []\n",
    "    for batch_x, batch_y in train_iter:\n",
    "        batch_x = batch_x.numpy()\n",
    "        batch_y = batch_y.numpy()\n",
    "        \n",
    "        loss, grad = value_and_grad(jit(Network.loss))(params, batch_x, batch_y)\n",
    "        \n",
    "        running_loss.append(loss)\n",
    "        \n",
    "        model.update(grad, lr)\n",
    "    \n",
    "    print(f\"Epoch {i} : {np.mean(running_loss)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9075356\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_t, Y_t)).shuffle(X_t.shape[0])\n",
    "test_iter = iter(test_dataset.batch(batch))\n",
    "\n",
    "acc = []\n",
    "for batch_x, batch_y in test_iter:\n",
    "        batch_x = batch_x.numpy()\n",
    "        batch_y = batch_y.numpy()\n",
    "        \n",
    "        y_pred = model.forward(batch_x)\n",
    "        y_pred = jnp.argmax(y_pred,1)\n",
    "        acc.append(jnp.mean(y_pred == jnp.argmax(batch_y,1)))\n",
    "        \n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
